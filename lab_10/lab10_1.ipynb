{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.3.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.6 (default, Mar 22 2014 22:59:56)\n",
      "SparkContext available as sc, HiveContext available as sqlCtx.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "execfile(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'cat': u'3/business_management|6/economics_finance',\n",
       " u'desc': u'This course introduces the basic financial statements used by most businesses, as well as the essential tools used to prepare them. This course will serve as a resource to help business students succeed in their upcoming university-level accounting classes, and as a refresher for upper division accounting students who are struggling to recall elementary concepts essential to more advanced accounting topics. Business owners will also benefit from this class by gaining essential skills necessary to organize and manage information pertinent to operating their business. At the conclusion of the class, students will understand the balance sheet, income statement, and cash flow statement. They will be able to differentiate between cash basis and accrual basis techniques, and know when each is appropriate. They\\u2019ll also understand the accounting equation, how to journalize and post transactions, how to adjust and close accounts, and how to prepare key financial reports. All material for this class is written and delivered by the professor, and can be previewed here. Students must have access to a spreadsheet program to participate.',\n",
       " u'id': 4,\n",
       " u'lang': u'en',\n",
       " u'name': u'Accounting Cycle: The Foundation of Business Measurement and Reporting',\n",
       " u'provider': u'Canvas Network'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_data = sc.textFile(\"/recsys/DO_record_per_line.json\")\n",
    "list_json = json_data.map(lambda x: json.loads(x))\n",
    "list_json.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat                  desc                 id lang name                 provider      \n",
      "3/business_manage... This course intro... 4  en   Accounting Cycle:... Canvas Network\n",
      "11/law               This online cours... 5  en   American Counter ... Canvas Network\n",
      "5/computer_scienc... This course is ta... 6  fr   Arithmétique: en ... Canvas Network\n",
      "14/social_sciences   We live in a digi... 7  en   Becoming a Dynami... Canvas Network\n",
      "2/biology_life_sc... This self-paced c... 8  en   Bioethics            Canvas Network\n",
      "9/humanities|15/m... This game-based c... 9  en   College Foundatio... Canvas Network\n",
      "14/social_sciences   What’s in your di... 10 en   Digital Literacies I Canvas Network\n",
      "14/social_sciences   The goal of the D... 11 en   Digital Literacie... Canvas Network\n",
      "14/social_sciences   Ready to explore ... 12 en   Digital Tools for... Canvas Network\n",
      "14/social_sciences   This self-paced c... 13 en   Discover Your Val... Canvas Network\n",
      "12/medicine_health   What is “interpro... 14 en   Enhancing Patient... Canvas Network\n",
      "16/languages         This course prese... 15 en   Ethics and Values... Canvas Network\n",
      "4/chemistry          Chemistry is an i... 16 en   Exploring Chemistry  Canvas Network\n",
      "8/engineering_tec... Are you consideri... 17 en   Exploring Enginee... Canvas Network\n",
      "1/arts_music_film    Princess stories ... 18 en   Fairy Tales: Orig... Canvas Network\n",
      "9/humanities         This first instal... 19 en   First Peoples to ... Canvas Network\n",
      "14/social_sciences   This course exami... 20 en   Forums for a Future  Canvas Network\n",
      "9/humanities         This course will ... 21 en   From the Gilded A... Canvas Network\n",
      "8/engineering_tec... The field of tech... 22 en   Fundamentals of S... Canvas Network\n",
      "14/social_sciences   Are you a Higher ... 23 en   Hybrid Courses: B... Canvas Network\n"
     ]
    }
   ],
   "source": [
    "df_json = sqlCtx.jsonFile(\"/recsys/DO_record_per_line.json\")\n",
    "df_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\"\"\"\"\n",
    "from nltk.stem.api import StemmerI\n",
    "from nltk.stem.regexp import RegexpStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.rslp import RSLPStemmer\n",
    "\"\"\"\n",
    "import re\n",
    "from pymystem3 import Mystem\n",
    "from bs4 import BeautifulSoup # удобная библиотека для обработки html-тегов, которые есть в текстах\n",
    "\n",
    "m = Mystem()\n",
    "#wnl = WordNetLemmatizer()\n",
    "swEn = nltk.corpus.stopwords.words('english')\n",
    "swRu = nltk.corpus.stopwords.words('russian')\n",
    "swEs = nltk.corpus.stopwords.words('spanish')\n",
    "reWords = re.compile(u'[áéíóúñüа-яёa-z\\d]+')#\\u00C0-\\u00FF \n",
    "\n",
    "def getWords(text,lang=\"en\",minLen=2):\n",
    "    text = text.strip()\n",
    "    if text > \"\":\n",
    "        text = BeautifulSoup(text).get_text() # удаляем html-теги\n",
    "\n",
    "        if lang == \"ru\":\n",
    "            words = m.lemmatize(text)\n",
    "            words = [token.lower() for token in words if any([c for c in token.strip() if c.isalpha()])]\n",
    "        else:\n",
    "            words = reWords.findall(text.lower())\n",
    "            words = [token for token in words if any([c for c in token.strip() if c.isalpha()])]\n",
    "\n",
    "        if minLen > 0:\n",
    "            words = [word for word in words if len(word) >= minLen]\n",
    "\n",
    "        if lang == 'en':\n",
    "            stopWords = swEn\n",
    "        elif lang == 'ru':\n",
    "            stopWords = swRu + swEn\n",
    "        elif lang == 'es':\n",
    "            stopWords = swEs\n",
    "        else:\n",
    "            stopWords = []\n",
    "\n",
    "        if len(stopWords) > 0: \n",
    "            words = [word for word in words if word not in stopWords]\n",
    "\n",
    "        if lang == \"en\":\n",
    "            #stemmer = PorterStemmer()\n",
    "            #stemmer = LancasterStemmer()\n",
    "            stemmer = SnowballStemmer(\"english\")\n",
    "            words = [stemmer.stem(t) for t in words]\n",
    "            #words = [wnl.lemmatize(t, pos='v') for t in words]\n",
    "            #words = [wnl.lemmatize(t) for t in words]\n",
    "        elif lang == \"es\":\n",
    "            stemmer = SnowballStemmer(\"spanish\")\n",
    "            words = [stemmer.stem(t) for t in words]\n",
    "        \"\"\"\n",
    "        elif lang == \"ru\":\n",
    "            stemmer = SnowballStemmer(\"russian\")\n",
    "            words = [stemmer.stem(t) for t in words]\n",
    "        \"\"\"\n",
    "    else:\n",
    "        words = ()\n",
    "    return words\n",
    "\n",
    "def getWords2(text,lang='en',minLen=2):\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    words = reWords.findall(text.lower())\n",
    "    words = [token for token in words if any([c for c in token.strip() if c.isalpha()])]\n",
    "    if minLen > 0:\n",
    "        words = [word for word in words if len(word) >= minLen]\n",
    "        \n",
    "    if lang == 'en':\n",
    "        stopWords = swEn\n",
    "    elif lang == 'ru':\n",
    "        stopWords = swRu + swEn\n",
    "    elif lang == 'es':\n",
    "        stopWords = swEs\n",
    "    else:\n",
    "        stopWords = []    \n",
    "    if len(stopWords) > 0: \n",
    "        words = [word for word in words if word not in stopWords]\n",
    "        \n",
    "    return words\n",
    "\n",
    "def getWords3(text):\n",
    "    return reWords.findall(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o561.fitIDF.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 66.0 failed 4 times, most recent failure: Lost task 0.3 in stage 66.0 (TID 189, ip-172-31-41-155.eu-west-1.compute.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar/pyspark/worker.py\", line 101, in main\n    process()\n  File \"/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar/pyspark/worker.py\", line 96, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar/pyspark/serializers.py\", line 236, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-37-512f5d458c05>\", line 19, in <lambda>\n  File \"<ipython-input-36-9fdb7c103869>\", line 29, in getWords\n  File \"/usr/local/lib/python2.7/dist-packages/pymystem3/mystem.py\", line 250, in lemmatize\n    infos = self.analyze(text)\n  File \"/usr/local/lib/python2.7/dist-packages/pymystem3/mystem.py\", line 235, in analyze\n    result.extend(self._analyze_impl(line))\n  File \"/usr/local/lib/python2.7/dist-packages/pymystem3/mystem.py\", line 264, in _analyze_impl\n    self._start_mystem()\n  File \"/usr/local/lib/python2.7/dist-packages/pymystem3/mystem.py\", line 217, in _start_mystem\n    close_fds=True if _POSIX else False)\n  File \"/usr/lib/python2.7/subprocess.py\", line 710, in __init__\n    errread, errwrite)\n  File \"/usr/lib/python2.7/subprocess.py\", line 1327, in _execute_child\n    raise child_exception\nOSError: [Errno 2] No such file or directory\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:98)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:94)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:144)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:201)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1157)\n\tat org.apache.spark.rdd.RDD$$anonfun$28.apply(RDD.scala:988)\n\tat org.apache.spark.rdd.RDD$$anonfun$28.apply(RDD.scala:988)\n\tat org.apache.spark.rdd.RDD$$anonfun$29.apply(RDD.scala:989)\n\tat org.apache.spark.rdd.RDD$$anonfun$29.apply(RDD.scala:989)\n\tat org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:634)\n\tat org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:634)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1191)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1191)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-512f5d458c05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mdoc_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0midf_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_tf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midf_fit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_tf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mnormalized_tf_idf_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalizer_l2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/mllib/feature.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dataset should be an RDD of term frequency vectors\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m         \u001b[0mjmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fitIDF\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminDocFreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_convert_to_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mIDFModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o561.fitIDF.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 66.0 failed 4 times, most recent failure: Lost task 0.3 in stage 66.0 (TID 189, ip-172-31-41-155.eu-west-1.compute.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar/pyspark/worker.py\", line 101, in main\n    process()\n  File \"/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar/pyspark/worker.py\", line 96, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar/pyspark/serializers.py\", line 236, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-37-512f5d458c05>\", line 19, in <lambda>\n  File \"<ipython-input-36-9fdb7c103869>\", line 29, in getWords\n  File \"/usr/local/lib/python2.7/dist-packages/pymystem3/mystem.py\", line 250, in lemmatize\n    infos = self.analyze(text)\n  File \"/usr/local/lib/python2.7/dist-packages/pymystem3/mystem.py\", line 235, in analyze\n    result.extend(self._analyze_impl(line))\n  File \"/usr/local/lib/python2.7/dist-packages/pymystem3/mystem.py\", line 264, in _analyze_impl\n    self._start_mystem()\n  File \"/usr/local/lib/python2.7/dist-packages/pymystem3/mystem.py\", line 217, in _start_mystem\n    close_fds=True if _POSIX else False)\n  File \"/usr/lib/python2.7/subprocess.py\", line 710, in __init__\n    errread, errwrite)\n  File \"/usr/lib/python2.7/subprocess.py\", line 1327, in _execute_child\n    raise child_exception\nOSError: [Errno 2] No such file or directory\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:98)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:94)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:144)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:201)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1157)\n\tat org.apache.spark.rdd.RDD$$anonfun$28.apply(RDD.scala:988)\n\tat org.apache.spark.rdd.RDD$$anonfun$28.apply(RDD.scala:988)\n\tat org.apache.spark.rdd.RDD$$anonfun$29.apply(RDD.scala:989)\n\tat org.apache.spark.rdd.RDD$$anonfun$29.apply(RDD.scala:989)\n\tat org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:634)\n\tat org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:634)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1191)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1191)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.feature import IDF\n",
    "from pyspark.mllib.feature import Normalizer\n",
    "\n",
    "def calc_cosine_similarity(a,b):\n",
    "    return a.dot(b)\n",
    "\n",
    "def fchosen_doc(idnum,df_rec=None):\n",
    "    if df_rec == None:\n",
    "        df_rec = df_json[df_json.id == idnum].first()\n",
    "    return normalizer_l2.transform(\n",
    "        idf_fit.transform(\n",
    "            tf.transform(\n",
    "                getWords(df_rec[4]+'. '+df_rec[1]+'. '+df_rec[0],df_rec[3]))))\n",
    "\n",
    "normalizer_l2 = Normalizer(p=2)\n",
    "tf, idf = HashingTF(), IDF()\n",
    "\n",
    "tokens = df_json.map(lambda x: getWords(x.name+'. '+x.desc+'. '+x.cat,x.lang))\n",
    "doc_tf = tf.transform(tokens)\n",
    "\n",
    "idf_fit = idf.fit(doc_tf)\n",
    "tfidf = idf_fit.transform(doc_tf)\n",
    "normalized_tf_idf_vectors = normalizer_l2.transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids = [8200]#14398, 24543, 12873, 13449, 8200, 1133]\n",
    "top11 = list()\n",
    "\n",
    "for idnum in ids:    \n",
    "    df_rec = df_json[df_json.id == idnum].first()\n",
    "    lang = df_rec[3]\n",
    "    \n",
    "    chosen_doc = fchosen_doc(idnum,df_rec)\n",
    "    #chosen_doc_br = sc.broadcast(chosen_doc)\n",
    "    similarities = normalized_tf_idf_vectors.map(lambda x: calc_cosine_similarity(x, chosen_doc))\n",
    "    \n",
    "    listid = df_json.map(lambda x: x.id).collect()\n",
    "    listok = similarities.collect()\n",
    "    ok = sorted(zip(listid,listok),key = lambda x: -x[1])[:11]\n",
    "    \n",
    "    #ok = df_json.map(lambda x: x.id).zip(similarities).sortBy(lambda x: -x[1]).take(11)\n",
    "\n",
    "    top11.append((idnum,lang,ok))\n",
    "    print idnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = getWords('')#df_json[df_json.id == 8200].first()[1],'ru')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "ok = dict()\n",
    "for val in top11:\n",
    "    s = str(val[0])\n",
    "    ok[s] = map(lambda x: x[0],filter(lambda x: x[0] != val[0],val[2]))\n",
    "out = json.dumps(ok, sort_keys=False)\n",
    "print out\n",
    "f = open(r\"lab10.json\",\"w\")\n",
    "f.write(out)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in top11:\n",
    "    print x[0],x[1]\n",
    "    for y in x[2]:\n",
    "        print '  ',y[0],',',y[1],':',df_json[df_json.id == y[0]].collect()[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aprende a tocar el teclado como los expertos by Virtuosso Producciones\n"
     ]
    }
   ],
   "source": [
    "print df_json[df_json.id == 12802].collect()[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aprende a tocar el piano desde cero by Virtuosso Producciones\n"
     ]
    }
   ],
   "source": [
    "print df_json[df_json.id == 12658].collect()[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{1,2} & {2,3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
