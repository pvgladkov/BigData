{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.3.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.6 (default, Mar 22 2014 22:59:56)\n",
      "SparkContext available as sc, HiveContext available as sqlCtx.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "execfile(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'cat': u'3/business_management|6/economics_finance',\n",
       " u'desc': u'This course introduces the basic financial statements used by most businesses, as well as the essential tools used to prepare them. This course will serve as a resource to help business students succeed in their upcoming university-level accounting classes, and as a refresher for upper division accounting students who are struggling to recall elementary concepts essential to more advanced accounting topics. Business owners will also benefit from this class by gaining essential skills necessary to organize and manage information pertinent to operating their business. At the conclusion of the class, students will understand the balance sheet, income statement, and cash flow statement. They will be able to differentiate between cash basis and accrual basis techniques, and know when each is appropriate. They\\u2019ll also understand the accounting equation, how to journalize and post transactions, how to adjust and close accounts, and how to prepare key financial reports. All material for this class is written and delivered by the professor, and can be previewed here. Students must have access to a spreadsheet program to participate.',\n",
       " u'id': 4,\n",
       " u'lang': u'en',\n",
       " u'name': u'Accounting Cycle: The Foundation of Business Measurement and Reporting',\n",
       " u'provider': u'Canvas Network'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_data = sc.textFile(\"/recsys/DO_record_per_line.json\")\n",
    "list_json = json_data.map(lambda x: json.loads(x))\n",
    "list_json.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat                  desc                 id lang name                 provider      \n",
      "3/business_manage... This course intro... 4  en   Accounting Cycle:... Canvas Network\n",
      "11/law               This online cours... 5  en   American Counter ... Canvas Network\n",
      "5/computer_scienc... This course is ta... 6  fr   Arithmétique: en ... Canvas Network\n",
      "14/social_sciences   We live in a digi... 7  en   Becoming a Dynami... Canvas Network\n",
      "2/biology_life_sc... This self-paced c... 8  en   Bioethics            Canvas Network\n",
      "9/humanities|15/m... This game-based c... 9  en   College Foundatio... Canvas Network\n",
      "14/social_sciences   What’s in your di... 10 en   Digital Literacies I Canvas Network\n",
      "14/social_sciences   The goal of the D... 11 en   Digital Literacie... Canvas Network\n",
      "14/social_sciences   Ready to explore ... 12 en   Digital Tools for... Canvas Network\n",
      "14/social_sciences   This self-paced c... 13 en   Discover Your Val... Canvas Network\n",
      "12/medicine_health   What is “interpro... 14 en   Enhancing Patient... Canvas Network\n",
      "16/languages         This course prese... 15 en   Ethics and Values... Canvas Network\n",
      "4/chemistry          Chemistry is an i... 16 en   Exploring Chemistry  Canvas Network\n",
      "8/engineering_tec... Are you consideri... 17 en   Exploring Enginee... Canvas Network\n",
      "1/arts_music_film    Princess stories ... 18 en   Fairy Tales: Orig... Canvas Network\n",
      "9/humanities         This first instal... 19 en   First Peoples to ... Canvas Network\n",
      "14/social_sciences   This course exami... 20 en   Forums for a Future  Canvas Network\n",
      "9/humanities         This course will ... 21 en   From the Gilded A... Canvas Network\n",
      "8/engineering_tec... The field of tech... 22 en   Fundamentals of S... Canvas Network\n",
      "14/social_sciences   Are you a Higher ... 23 en   Hybrid Courses: B... Canvas Network\n"
     ]
    }
   ],
   "source": [
    "df_json = sqlCtx.jsonFile(\"/recsys/DO_record_per_line.json\")\n",
    "df_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.feature import IDF\n",
    "from pyspark.mllib.feature import Normalizer\n",
    "\n",
    "def calc_cosine_similarity(a,b):\n",
    "    return a.dot(b)\n",
    "\n",
    "normalizer_l2 = Normalizer(p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\"\"\"\"\n",
    "from nltk.stem.api import StemmerI\n",
    "from nltk.stem.regexp import RegexpStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.rslp import RSLPStemmer\n",
    "\"\"\"\n",
    "import re\n",
    "from pymystem3 import Mystem\n",
    "from bs4 import BeautifulSoup # удобная библиотека для обработки html-тегов, которые есть в текстах\n",
    "\n",
    "m = Mystem()\n",
    "#wnl = WordNetLemmatizer()\n",
    "swEn = nltk.corpus.stopwords.words('english')\n",
    "swRu = nltk.corpus.stopwords.words('russian')\n",
    "swEs = nltk.corpus.stopwords.words('spanish')\n",
    "reWords = re.compile(u'[а-яёa-zóñúüéíá\\d]+')\n",
    "\n",
    "def getWords(text,lang=\"en\",minLen=2):\n",
    "    text = BeautifulSoup(text).get_text() # удаляем html-теги\n",
    "   \n",
    "    if lang == \"ru\":\n",
    "        words = m.lemmatize(text)\n",
    "        words = [token.lower() for token in words if any([c for c in token.strip() if c.isalpha()])]\n",
    "    else:\n",
    "        words = reWords.findall(text.lower())\n",
    "        words = [token for token in words if any([c for c in token.strip() if c.isalpha()])]\n",
    "    \n",
    "    if minLen > 0:\n",
    "        words = [word for word in words if len(word) >= minLen]\n",
    "        \n",
    "    if lang == 'en':\n",
    "        stopWords = swEn\n",
    "    elif lang == 'ru':\n",
    "        stopWords = swRu\n",
    "    elif lang == 'es':\n",
    "        stopWords = swEs\n",
    "    else:\n",
    "        stopWords = []\n",
    "    \n",
    "    if len(stopWords) > 0: \n",
    "        words = [word for word in words if word not in stopWords]\n",
    "        \n",
    "    if lang == \"en\":\n",
    "        #stemmer = PorterStemmer()\n",
    "        #stemmer = LancasterStemmer()\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        words = [stemmer.stem(t) for t in words]\n",
    "        #words = [wnl.lemmatize(t, pos='v') for t in words]\n",
    "        #words = [wnl.lemmatize(t) for t in words]\n",
    "    elif lang == \"es\":\n",
    "        stemmer = SnowballStemmer(\"spanish\")\n",
    "        words = [stemmer.stem(t) for t in words]\n",
    "    \"\"\"\n",
    "    elif lang == \"ru\":\n",
    "        stemmer = SnowballStemmer(\"russian\")\n",
    "        words = [stemmer.stem(t) for t in words]\n",
    "    \"\"\"\n",
    "    return words\n",
    "\n",
    "def getWords2(text,lang='en',minLen=2):\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    words = reWords.findall(text.lower())\n",
    "    words = [token for token in words if any([c for c in token.strip() if c.isalpha()])]\n",
    "    if minLen > 0:\n",
    "        words = [word for word in words if len(word) >= minLen]\n",
    "        \n",
    "    if lang == 'en':\n",
    "        stopWords = swEn\n",
    "    elif lang == 'ru':\n",
    "        stopWords = swRu\n",
    "    elif lang == 'es':\n",
    "        stopWords = swEs\n",
    "    else:\n",
    "        stopWords = []    \n",
    "    if len(stopWords) > 0: \n",
    "        words = [word for word in words if word not in stopWords]\n",
    "        \n",
    "    return words\n",
    "\n",
    "def getWords3(text):\n",
    "    return reWords.findall(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14398\n",
      "24543\n",
      "12873\n",
      "13449\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o341.fitIDF.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 4 times, most recent failure: Lost task 0.3 in stage 25.0 (TID 50, ip-172-31-23-82.eu-west-1.compute.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/data/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar/pyspark/worker.py\", line 101, in main\n    process()\n  File \"/data/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar/pyspark/worker.py\", line 96, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/data/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar/pyspark/serializers.py\", line 236, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-6-5c2b8434d77b>\", line 19, in <lambda>\n  File \"<ipython-input-5-47be7afd73ec>\", line 27, in getWords\n  File \"/usr/local/lib/python2.7/dist-packages/pymystem3/mystem.py\", line 250, in lemmatize\n    infos = self.analyze(text)\n  File \"/usr/local/lib/python2.7/dist-packages/pymystem3/mystem.py\", line 235, in analyze\n    result.extend(self._analyze_impl(line))\n  File \"/usr/local/lib/python2.7/dist-packages/pymystem3/mystem.py\", line 264, in _analyze_impl\n    self._start_mystem()\n  File \"/usr/local/lib/python2.7/dist-packages/pymystem3/mystem.py\", line 217, in _start_mystem\n    close_fds=True if _POSIX else False)\n  File \"/usr/lib/python2.7/subprocess.py\", line 710, in __init__\n    errread, errwrite)\n  File \"/usr/lib/python2.7/subprocess.py\", line 1327, in _execute_child\n    raise child_exception\nOSError: [Errno 13] Permission denied\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1191)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1191)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5c2b8434d77b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mdoc_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc_tf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0midf_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_tf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midf_fit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_tf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mnormalized_tf_idf_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalizer_l2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/mllib/feature.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dataset should be an RDD of term frequency vectors\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m         \u001b[0mjmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fitIDF\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminDocFreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_convert_to_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mIDFModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o341.fitIDF.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 4 times, most recent failure: Lost task 0.3 in stage 25.0 (TID 50, ip-172-31-23-82.eu-west-1.compute.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/data/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar/pyspark/worker.py\", line 101, in main\n    process()\n  File \"/data/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar/pyspark/worker.py\", line 96, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/data/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar/pyspark/serializers.py\", line 236, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-6-5c2b8434d77b>\", line 19, in <lambda>\n  File \"<ipython-input-5-47be7afd73ec>\", line 27, in getWords\n  File \"/usr/local/lib/python2.7/dist-packages/pymystem3/mystem.py\", line 250, in lemmatize\n    infos = self.analyze(text)\n  File \"/usr/local/lib/python2.7/dist-packages/pymystem3/mystem.py\", line 235, in analyze\n    result.extend(self._analyze_impl(line))\n  File \"/usr/local/lib/python2.7/dist-packages/pymystem3/mystem.py\", line 264, in _analyze_impl\n    self._start_mystem()\n  File \"/usr/local/lib/python2.7/dist-packages/pymystem3/mystem.py\", line 217, in _start_mystem\n    close_fds=True if _POSIX else False)\n  File \"/usr/lib/python2.7/subprocess.py\", line 710, in __init__\n    errread, errwrite)\n  File \"/usr/lib/python2.7/subprocess.py\", line 1327, in _execute_child\n    raise child_exception\nOSError: [Errno 13] Permission denied\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1191)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1191)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
     ]
    }
   ],
   "source": [
    "ids = [14398, 24543, 12873, 13449, 8200, 1133]\n",
    "top11 = list()\n",
    "\n",
    "def fchosen_doc(idnum,df_rec=None):\n",
    "    if df_rec == None:\n",
    "        df_rec = df_json[df_json.id == idnum].first()\n",
    "    return normalizer_l2.transform(\n",
    "        idf_fit.transform(\n",
    "            tf.transform(getWords(\n",
    "                    df_rec[4]+'. '+df_rec[1]+'. '+df_rec[0],lang))))\n",
    "    #return doc_tfIDs.zip(normalized_tf_idf_vectors).filter(lambda x: x[0] == idnum).first()[1]\n",
    "\n",
    "for idnum in ids:\n",
    "    \n",
    "    df_rec = df_json[df_json.id == idnum].first()\n",
    "    lang = df_rec[3]\n",
    "    tf, idf = HashingTF(2**18), IDF()\n",
    "\n",
    "    doc_tf = df_json.filter(df_json.lang==lang)\\\n",
    "    .map(lambda x: (x.id,tf.transform(getWords(x.name+'. '+x.desc+'. '+x.cat,lang))))\n",
    "    doc_tfIDs = doc_tf.map(lambda x: x[0])\n",
    "    doc_tf = doc_tf.map(lambda x: x[1])\n",
    "\n",
    "    idf_fit = idf.fit(doc_tf)\n",
    "    tfidf = idf_fit.transform(doc_tf)\n",
    "    normalized_tf_idf_vectors = normalizer_l2.transform(tfidf)\n",
    "\n",
    "    chosen_doc = fchosen_doc(idnum,df_rec)\n",
    "    similarities = normalized_tf_idf_vectors.map(lambda x: calc_cosine_similarity(x, chosen_doc))\n",
    "    try:\n",
    "        ok = sorted(zip(doc_tfIDs.collect(),similarities.collect()),key = lambda x: -x[1])[:11]\n",
    "        #doc_tfIDs.zip(similarities).sortBy(lambda x: -x[1]).take(11)\n",
    "        top11.append((idnum,lang,ok))\n",
    "    except:\n",
    "        print '?'\n",
    "    print idnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"12873\": [12802, 12658, 12807, 13082, 12801, 19160, 13080, 12806, 12800, 12803], \"1133\": [1128, 1120, 51, 1117, 8211, 19614, 1214, 1126, 876, 1141], \"8200\": [8201, 932, 1116, 1117, 8212, 872, 1032, 20297, 1340, 1128], \"24543\": [25074, 19804, 22612, 4504, 7660, 17925, 25581, 18949, 16604, 9799], \"13449\": [13152, 5690, 23472, 22739, 10034, 20277, 5749, 7250, 25010, 12863], \"14398\": [25782, 13781, 7153, 26507, 17499, 18847, 13348, 27951, 15909, 13665]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "ok = dict()\n",
    "for val in top11:\n",
    "    s = str(val[0])\n",
    "    ok[s] = map(lambda x: x[0],filter(lambda x: x[0] != val[0],val[2]))\n",
    "out = json.dumps(ok, sort_keys=False)\n",
    "print out\n",
    "f = open(r\"lab10.json\",\"w\")\n",
    "f.write(out)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14398 en\n",
      "   14398 , 1.0 : CSS with LESS and Sass\n",
      "   25782 , 0.613474917418 : Write Clean CSS using SASS - Udemy\n",
      "   13781 , 0.605405930015 : Assembling Sass\n",
      "   7153 , 0.602583786135 : Step by Step Sass by Lisa Catalano\n",
      "   26507 , 0.59662219653 : SASS - Beginner Crash Course - Udemy\n",
      "   17499 , 0.552075360193 : Learning Sass - Syntactically Awesome StyleSheets\n",
      "   18847 , 0.551727418558 : Advanced CSS Development by RefactorU LLC\n",
      "   13348 , 0.551177296819 : Learning Sass - CSS Just Got Exciting. by Infinite Skills\n",
      "   27951 , 0.542377529767 : CSS : CSS for beginners - Udemy\n",
      "   15909 , 0.527014155187 : Responsive CSS with Sass and Compass\n",
      "   13665 , 0.522096137484 : The Next Step with Sass and Compass by Lisa Catalano\n",
      "24543 en\n",
      "   24543 , 1.0 : Love Your Life - Why Settle? Be Happy - Dream Builder Course\n",
      "   25074 , 0.379809868876 : Reach your dreams w/ 7 simple steps that you can DO TODAY!\n",
      "   19804 , 0.371640489565 : How to Remember Your Dreams by Anthony Metivier\n",
      "   22612 , 0.368647394074 : Achieve Your Goals - Achieve Your Dreams! - Udemy\n",
      "   4504 , 0.36140524615 : Change Your Thinking Change Your Life: By Jim Donovan\n",
      "   7660 , 0.361157524402 : 7 Steps: Vision to Action by Terry Igharoro\n",
      "   17925 , 0.357955901376 : Dream Interpretation Made Easy - In One Hour! by Dave Lappin\n",
      "   25581 , 0.349890308363 : Transform Your Life Masterclass - Udemy\n",
      "   18949 , 0.349485211704 : Spice up your love life now through NLP by Pradeep Aggarwal\n",
      "   16604 , 0.342306484374 : Success Factors to Create Your Dream Life by Janet Diaz\n",
      "   9799 , 0.331441078796 : How To Achieve Your Dreams and Live The Life You Deserve by Tamara P\n",
      "12873 es\n",
      "   12873 , 1.0 : Aprende a Tocar Teclado - Piano by Carlos Timana Ortega\n",
      "   12802 , 0.3468273499 : Aprende a tocar el teclado como los expertos by Virtuosso Producciones\n",
      "   12658 , 0.264514466572 : Aprende a tocar el piano desde cero by Virtuosso Producciones\n",
      "   12807 , 0.21793451305 : Aprende todos los secretos de la salsa para piano by Virtuosso Producciones\n",
      "   13082 , 0.20142239762 : Aprende a tocar ritmo pop en el piano y teclado by Virtuosso Producciones\n",
      "   12801 , 0.193826992177 : Aprende a tocar blues en el piano y teclado by Virtuosso Producciones\n",
      "   19160 , 0.191402660933 : Aprende a tocar el Acordeón \"de oído y con técnica\" by Néstor José León Ojeda\n",
      "   13080 , 0.19069729092 : Aprende a tocar Música Cristiana en el piano y teclado by Virtuosso Producciones\n",
      "   12806 , 0.184153677805 : Aprende a tocar ritmos latinos en el teclado by Virtuosso Producciones\n",
      "   12800 , 0.179815445288 : Ahora aprende a tocar jazz en el piano by Virtuosso Producciones\n",
      "   12803 , 0.160007023951 : Aprende a tocar Jazz y Ritmos Latinos en la guitarra by Virtuosso Producciones\n",
      "13449 es\n",
      "   13449 , 1.0 : Gestión avanzada de datos con MongoDB by José Antonio Sánchez Ortiz\n",
      "   13152 , 0.500202005788 : Primeros pasos en MongoDB by José Antonio Sánchez Ortiz\n",
      "   5690 , 0.140513663776 : eXpresiones en Adobe After Effects - 401 by Jorge Mochon\n",
      "   23472 , 0.126636584169 : Enriquece tu documento Word con hipervínculos y referencias\n",
      "   22739 , 0.122547646965 : Aprende expresiones lambda con Java 8 fácilmente.\n",
      "   10034 , 0.102265555208 : Introducción a Oracle SQL by Ing. Clarisa Maman Orfali\n",
      "   20277 , 0.102034128886 : Tesis: del planteo del índice a su redacción by Lic. Gabriela Gómez Del Río\n",
      "   5749 , 0.100602146514 : Fundamentos de programación con PHP y MySQL by Francisco Javier Arce Anguiano\n",
      "   7250 , 0.0978206629707 : Python Acelerado by Tomas Bradanovic\n",
      "   25010 , 0.0868782534243 : Programador web: PHP y MySql Profesional ¡Fácil y Práctico!\n",
      "   12863 , 0.0829480270486 : Microsoft Office 2010 by Etraining S.A DE C.V\n",
      "8200 ru\n",
      "   8200 , 1.0 : Введение в отказоустойчивые технологии высокопроизводительных вычислительных систем (суб)микронного, супрамолекулярного и нанометрового диапазона\n",
      "   8201 , 0.19313127051 : Программно-аппаратные платформы и вычислительные наноструктуры\n",
      "   932 , 0.172999619261 : Принципы построения и функционирования ЭВМ\n",
      "   1116 , 0.121224466394 : Основы технологии локальных сетей\n",
      "   1117 , 0.110778559415 : Основы локальных сетей\n",
      "   8212 , 0.103184269642 : Основы операционных систем\n",
      "   872 , 0.0952117793286 : Введение в анализ, синтез и моделирование систем\n",
      "   1032 , 0.0861000903928 : Анализ требований к автоматизированным информационным системам\n",
      "   20297 , 0.0861000903928 : Анализ требований к автоматизированным информационным системам\n",
      "   1340 , 0.0832819982437 : Введение в разработку для Windows Phone\n",
      "   1128 , 0.0827731894324 : Локальные сети и интернет\n",
      "1133 ru\n",
      "   1133 , 1.0 : Основы сетей передачи данных\n",
      "   1128 , 0.346285780484 : Локальные сети и интернет\n",
      "   1120 , 0.251486883515 : Абонентские сети доступа и технологии высокоскоростных сетей\n",
      "   51 , 0.24794971743 : Распределенные системы хранения и обработки данных\n",
      "   1117 , 0.23641147301 : Основы локальных сетей\n",
      "   8211 , 0.227122750189 : Построение коммутируемых компьютерных сетей\n",
      "   19614 , 0.221260784224 : Информация и данные\n",
      "   1214 , 0.211917935917 : Администрирование сетей Microsoft Windows XP Professional\n",
      "   1126 , 0.211881404365 : Основные протоколы интернет\n",
      "   876 , 0.210775733539 : Нейрокомпьютерные системы\n",
      "   1141 , 0.207288067213 : Сети Wi-Fi. Компания TRENDnet\n"
     ]
    }
   ],
   "source": [
    "for x in top11:\n",
    "    print x[0],x[1]\n",
    "    for y in x[2]:\n",
    "        print '  ',y[0],',',y[1],':',df_json[df_json.id == y[0]].collect()[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GetWords2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getWords('sadasd asdasd Фывфыв фывфыв лвлвлв ASAS 213123 12FB ','en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
